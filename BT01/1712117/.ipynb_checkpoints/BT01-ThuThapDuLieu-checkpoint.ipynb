{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# BT01: Thu thập dữ liệu\n",
    "\n",
    "Họ tên: Nguyễn Huỳnh Thảo Nhi\n",
    "\n",
    "MSSV: 1712117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "## Cách làm bài và nộp bài\n",
    "&#9889; Bạn lưu ý là mình sẽ dùng chương trình hỗ trợ chấm bài nên bạn cần phải tuân thủ chính xác qui định mà mình đặt ra, nếu không rõ thì hỏi, chứ không nên tự tiện làm theo ý của cá nhân.\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này. Đầu tiên, bạn điền họ tên và MSSV vào phần đầu file ở bên trên. Trong file, bạn làm bài ở những chỗ có ghi là:\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "hoặc đối với những phần code không bắt buộc thì là:\n",
    "```python\n",
    "# YOUR CODE HERE (OPTION)\n",
    "```\n",
    "hoặc đối với markdown cell thì là:\n",
    "```markdown\n",
    "YOUR ANSWER HERE\n",
    "```\n",
    "Tất nhiên, khi làm thì bạn xóa dòng `raise NotImplementedError()` đi.\n",
    "Đối những phần yêu cầu code thì thường ở ngay phía dưới sẽ có một (hoặc một số) cell chứa các bộ test để giúp bạn biết đã code đúng hay chưa; nếu chạy cell này không có lỗi gì thì có nghĩa là qua được các bộ test. Trong một số trường hợp, các bộ test có thể sẽ không đầy đủ; nghĩa là, nếu không qua được test thì là code sai, nhưng nếu qua được test thì chưa chắc đã đúng.\n",
    "\n",
    "Trong khi làm bài, bạn có thể cho in ra màn hình, tạo thêm các cell để test. Nhưng khi nộp bài thì bạn xóa các cell mà bạn tự tạo, xóa hoặc comment các câu lệnh in ra màn hình. Bạn lưu ý <font color=red>không được tự tiện xóa các cell hay sửa code của Thầy</font> (trừ những chỗ được phép sửa như đã nói ở trên).\n",
    "\n",
    "Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "\n",
    "*Nên nhớ mục tiêu chính ở đây là <font color=green>học, học một cách chân thật</font>. Bạn có thể thảo luận ý tưởng với bạn khác, nhưng <font color=green>code và bài làm phải là của bạn, dựa trên sự hiểu thật sự của bạn</font>. <font color=red>Nếu vi phạm thì sẽ bị 0 điểm cho toàn bộ môn học.</font>*\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Kernel` - `Restart Kernel & Run All Cells`, để restart và chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Kernel` - `Restart Kernel & Run All Cells` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, bạn tạo thư mục nộp bài theo cấu trúc sau:\n",
    "- Thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`)\n",
    "    - Thư mục `BT01`\n",
    "        - File `BT01-ThuThapDuLieu.ipynb` (không cần nộp các file khác)\n",
    "\n",
    "Cuối cùng, bạn nén thư mục `MSSV` này lại và nộp ở link trên moodle. <font color=red>Bạn lưu ý tuân thủ chính xác cấu trúc này.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_html in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: requests in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (2.22.0)\n",
      "Requirement already satisfied: w3lib in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (1.22.0)\n",
      "Requirement already satisfied: fake-useragent in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (0.1.11)\n",
      "Requirement already satisfied: parse in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (1.18.0)\n",
      "Requirement already satisfied: bs4 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (0.0.1)\n",
      "Requirement already satisfied: pyquery in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (1.4.1)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests_html) (0.2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests->requests_html) (1.25.10)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests->requests_html) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests->requests_html) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from requests->requests_html) (2.8)\n",
      "Requirement already satisfied: six>=1.4.1 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from w3lib->requests_html) (1.12.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from bs4->requests_html) (4.8.0)\n",
      "Requirement already satisfied: lxml>=2.1 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from pyquery->requests_html) (4.4.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from pyquery->requests_html) (1.1.0)\n",
      "Requirement already satisfied: websockets<9.0,>=8.1 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from pyppeteer>=0.0.14->requests_html) (8.1)\n",
      "Requirement already satisfied: pyee<8.0.0,>=7.0.1 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from pyppeteer>=0.0.14->requests_html) (7.0.4)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from pyppeteer>=0.0.14->requests_html) (4.50.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/apple/opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4->requests_html) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install requests_html\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "import requests\n",
    "import json\n",
    "import time # Dùng để sleep chương trình\n",
    "import pandas as pd # Dùng để đọc và hiển thị file csv (Pandas sẽ được học chi tiết ở buổi tới)\n",
    "import datetime as dt # Dùng để xử lý dữ liệu thời gian\n",
    "import re\n",
    "# YOUR CODE HERE (OPTION) \n",
    "# Nếu cần các thư viện khác thì bạn có thể import ở đây"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "## 1. Thu thập dữ liệu từ web bằng cách parse HTML (5đ)\n",
    "[Coursera](https://coursera.org) là trang cung cấp các khóa học online, là một nguồn tài liệu hữu ích để học tập. Trong phần này, bạn sẽ thu thập dữ liệu về các khóa học Data Science ở Coursera. \n",
    "\n",
    "Cụ thể là bạn sẽ viết hàm `collect_courses` ở bên dưới. Hàm này có các input như sau:\n",
    "- `course_urls_file`: là chuỗi, cho biết tên của file txt chứa các đường link đến các khóa học của Coursera (tất cả các khóa học, chứ không phải chỉ các khóa học về Data Science), mỗi đường link nằm trên một dòng; bạn sẽ đi theo đường link để lấy thông tin của khóa học. File này đã được cung cấp cho bạn: file \"course_urls.txt\" (mình tạo ra file này bằng cách parse [trang XML này](https://www.coursera.org/sitemap~www~courses.xml)). Theo file [robots.txt](https://www.coursera.org/robots.txt) thì các link trong file này đều được phép thu thập (bạn kiểm tra lại xem có đúng không). \n",
    "- `key_words`: là list, cho biết các từ khóa để lọc ra các link cần lấy. Ví dụ: `key_words=['data-science', 'datascience']` sẽ chỉ thu thập thông tin từ các link mà có từ \"data-science\" HOẶC \"datascience\" (chẳng hạn: https://www.coursera.org/learn/spatial-data-science).\n",
    "- `sleep_time`: là số thực, cho biết sau mỗi lần gửi request và parse lấy dữ liệu thì sẽ sleep bao nhiêu giây trước khi gửi tiếp lần request kế (sleep như vậy để tránh gửi quá nhiều request lên Coursera trong một thời gian ngắn). Trong hàm `collect_courses` bên dưới mình để giá trị mặc định là 1 giây. Trong Python, bạn sleep chương trình bằng câu lệnh `time.sleep(số giây)`.\n",
    "- `courses_file`: là chuỗi, cho biết tên của file csv dùng để lưu dữ liệu thu thập được; đây là file output (file này không có sẵn mà sẽ được tạo ra sau khi chạy hàm của bạn). Mình có cung cấp file output đúng \"correct_courses.csv\" (khi gọi hàm `collect_courses` với `key_words=['data-science', 'datascience']`) để bạn hình dung được nội dung cần có của file output. File này có các phần tử trên mỗi dòng được phân tách nhau bởi tab (`\\t`), và file này mã hóa utf-8. Bạn có thể xem file csv này ở JupyterLab bằng cách double-click vào file ở phía bên trái JupyterLab rồi chọn \"Delimiter\" là tab; hoặc xem ở dạng text bằng cách chuột phải vào file rồi \"Open With\", \"Editor\". File này gồm có các cột:\n",
    "    - `url`: đường link đến khóa học.\n",
    "    - `title`: tựa đề của khóa học.\n",
    "    - `provider`: trường cung cấp khóa học.\n",
    "    - `rating`: điểm số của khóa học (tại thời điểm bạn thu thập dữ liệu thì giá trị của cột này có thể sẽ hơi khác so với file output đúng của mình).\n",
    "    - `num_ratings`: số lượng người đã cho điểm cho khóa học (tại thời điểm bạn thu thập dữ liệu thì giá trị của cột này có thể sẽ hơi khác so với file output đúng của mình)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "925ed984709d1910bc53a87d128584ef",
     "grade": false,
     "grade_id": "cell-dc3850a3d53d80be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_courses(course_urls_file, key_words, courses_file, sleep_time=1):\n",
    "    # YOUR CODE HERE\n",
    "    course_urls = open(course_urls_file)\n",
    "    session = HTMLSession()\n",
    "\n",
    "    urls = []\n",
    "    titles = []\n",
    "    providers = []\n",
    "    ratings = []\n",
    "    num_ratings = []\n",
    "    \n",
    "    for course_url in course_urls:\n",
    "        flag = False\n",
    "        for key in key_words:\n",
    "            if (flag == True):\n",
    "                break\n",
    "            if (key in course_url.rstrip()):\n",
    "                flag = True\n",
    "        if (flag == False):\n",
    "            continue\n",
    "\n",
    "        r = session.get(course_url.rstrip())\n",
    "        \n",
    "        if (r.html.find('.banner-title', first=True) == None):\n",
    "            urls.append(course_url.rstrip())\n",
    "            titles.append('')\n",
    "            providers.append('')\n",
    "            ratings.append('')\n",
    "            num_ratings.append('')\n",
    "            continue\n",
    "            \n",
    "        urls.append(course_url.rstrip())\n",
    "        titles.append(r.html.find('.banner-title', first=True).text)\n",
    "        providers.append(r.html.find('.rc-Partner__title', first=True).text)\n",
    "        \n",
    "        _rating = r.html.find('.number-rating', first=True) \n",
    "        if (_rating != None):\n",
    "            ratings.append(re.findall(\"\\d+[.,]\\d*|\\d+\", _rating.text)[0])\n",
    "        else:\n",
    "            ratings.append(0)\n",
    "            \n",
    "        _num_ratings = r.html.find('.ratings-count-expertise-style', first=True)\n",
    "        \n",
    "        if (_num_ratings != None):\n",
    "            temp_num_ratings = re.findall(\"\\d+[.,]\\d*|\\d+\", _num_ratings.text)[0]\n",
    "            num_ratings.append(temp_num_ratings)\n",
    "        else:\n",
    "            num_ratings.append(0)\n",
    "        \n",
    "        time.sleep(sleep_time)\n",
    "      \n",
    "    df = pd.DataFrame(list(zip(urls, titles, providers, ratings, num_ratings)), columns= ['url', 'title', 'providers', 'ratings','num_ratings'])\n",
    "    df.to_csv(courses_file, encoding='utf-8', sep='\\t', index=False)\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ff72fc37c169d35422583361e59e377",
     "grade": true,
     "grade_id": "cell-f7ba8a08f45c6cf9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'course_urls.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8f58f8d3c8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcollect_courses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'course_urls.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'data-science'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datascience'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'courses.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcourses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'courses.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-87a46213709a>\u001b[0m in \u001b[0;36mcollect_courses\u001b[0;34m(course_urls_file, key_words, courses_file, sleep_time)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_courses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourse_urls_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourses_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcourse_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourse_urls_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'course_urls.txt'"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "collect_courses('course_urls.txt', ['data-science', 'datascience'], 'courses.csv')\n",
    "courses = pd.read_csv('courses.csv', sep='\\t')\n",
    "assert courses.shape == (26, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62a3bb1c02fe708c3dff8e41460d4c78",
     "grade": false,
     "grade_id": "cell-69bae8100b51ecac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_urls = open('course_urls.txt', 'r+')\n",
    "print(course_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## 2. Thu thập dữ liệu từ web bằng cách dùng web API (5đ)\n",
    "Trong phần này, bạn sẽ dùng GitHub API để thu thập các thùng chứa về Data Science. Để đơn giản, bạn sẽ dùng API không chứng thực (không cần đăng ký tài khoản, không cần key) của GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2.1. Thu thập ít hơn hoặc bằng 1000 thùng chứa (2.5đ)\n",
    "Đầu tiên, bạn sẽ viết hàm `collect_repositories` ở bên dưới. Hàm này có các input:\n",
    "- `start_date` và `end_date`: là chuỗi, cho biết là muốn tìm các thùng chứa được tạo từ ngày nào đến ngày nào (bao gồm cả 2 đầu). Ví dụ, `start_date='2017-01-01'` (năm-tháng-ngày) và `end_date='2017-01-31'`.\n",
    "- `keyword`: là chuỗi, cho biết là muốn tìm kiếm các thùng chứa với từ khóa nào (xuất hiện ở tên thùng chứa hoặc phần mô tả của thùng chứa). Ví dụ, `keyword='data science'`.\n",
    "- `per_page`: là số nguyên, cho biết là muốn bao nhiêu kết quả trên một page. Hiện giờ GitHub cho tối đa là `per_page=100`.\n",
    "\n",
    "Trong hàm này bạn sẽ vào đường link `f'https://api.github.com/search/repositories?q={keyword} created:{start_date}..{end_date}&per_page={per_page}'` (dạng f-string trong Python) để thu thập dữ liệu. Ví dụ, bạn có thể vào thử ở web browser: https://api.github.com/search/repositories?q=data%20science%20created:2017-01-01..2017-01-31&per_page=100 (khi nhập một đường link vào web browser và enter thì một số ký tự sẽ bị mã hóa; ví dụ khoảng trắng sẽ bị mã hóa là %20 hoặc +), hoặc dùng thư viện Requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Thư viện requests sẽ tự động mã hóa khoảng trắng cho bạn\n",
    "# Bạn có thể xem đường link sau khi đã mã hóa bằng câu lệnh: r.url\n",
    "r = requests.get('https://api.github.com/search/repositories?q=data science created:2017-01-01..2017-01-31&per_page=100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Xem 100 ký tự đầu tiên trong chuỗi JSON lấy được\n",
    "print(r.text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bạn thấy \"total_count\" có giá trị là 1615, nghĩa là tìm được 1615 thùng chứa. Tuy nhiên, đây chỉ là trang đầu tiên và chỉ có 100 thùng chứa (số lượng phần tử của \"items\"). Link tới trang kế tiếp nằm trong headers của nội dung gửi về từ server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "r.headers['Link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Link tới trang kế tiếp là link mà ở bên cạnh phía phải có từ `rel=\"next\"`. `rel=\"last\"` nghĩa là trang cuối. Vì đây là trang đầu tiên nên chỉ có \"next\" và \"last\", trang ở giữa còn có thêm \"first\" (trang đầu) và \"prev\" (trang trước). Bạn có thể xem thêm ở [document](https://developer.github.com/v3/guides/traversing-with-pagination/).\n",
    "\n",
    "Nếu để ý thì bạn thấy trang cuối là `page=10`, nghĩa là tất cả các trang chỉ có $10\\times100=1000$ thùng chứa. Trong khi đó, \"total_count\" là 1615. Lý do: \"the GitHub Search API provides up to **1,000 results for each search**\" ([document](https://developer.github.com/v3/search/)). Trong hàm này chỉ yêu cầu bạn lấy hết kết quả ở tất cả các trang là được ($\\le1000$ kết quả). Việc lấy $>1000$ kết quả sẽ làm ở phần kế tiếp.\n",
    "\n",
    "Một yêu cầu nữa trong hàm này là khi hết số lượng request được phép trong 1 phút (với API không chứng thực thì hiện nay được 10 request / phút) thì bạn phải cho chương trình đợi cho đến khi được reset lại (thường thì không cần đợi tới 1 phút vì thời gian 1 phút là tính từ lần request đầu tiên). Cách làm là bạn sẽ cho chương trình sleep 1 giây rồi dậy xem được reset chưa, nếu chưa thì lại sleep 1 giây rồi dậy ... Bạn lưu ý là, <font color=red>không được giả định lần request đầu tiên là chắc chắn thành công</font> (vì khi chấm bài mình sẽ phải chạy nhiều bài và có thể đến bài của bạn đã hết lượt request được phép). Để xem số lượng request còn lại, bạn có thể xem header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "r.headers['X-RateLimit-Remaining'] # Lưu ý kết quả trả về là chuỗi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Khi số lượng request còn lại bằng 0 thì có 2 trường hợp có thể xảy ra:\n",
    "- Lần request vừa xong thành công (là lần request cuối cùng được phép), lúc này `r.ok` bằng `True`.\n",
    "- Lần request vừa xong không thành công (đã hết lượng request được phép), lúc này `r.ok` bằng `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Output trả về của hàm `collect_repositories`:\n",
    "- `repositories`: là list các thùng chứa lấy được (list gồm \"item\" của tất cả các trang)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20cb81a6aca154078aed4053c325d8ec",
     "grade": false,
     "grade_id": "cell-62a49f8fda9cd909",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_repositories(start_date, end_date, keyword='data science', per_page=100):\n",
    "    repositories = []\n",
    "    # YOUR CODE HERE\n",
    "    url = f'https://api.github.com/search/repositories?q={keyword}created:{start_date}..{end_date}&per_page={per_page}'\n",
    "    nextLinkRequest = requests.get(url)\n",
    "    \n",
    "    remain = int(nextLinkRequest.headers['X-RateLimit-Remaining'])\n",
    "    while (remain == 0):\n",
    "        time.sleep(int(1))\n",
    "        nextLinkRequest = requests.get(url)\n",
    "        remain = int(nextLinkRequest.headers['X-RateLimit-Remaining'])\n",
    "\n",
    "    items = json.loads(nextLinkRequest.text)['items']\n",
    "    \n",
    "    for item in items:\n",
    "        repositories.append(item)\n",
    "    \n",
    "    while (True):\n",
    "        nextLinkUrl = re.findall(\"(?<=<)(.*)(?=>; rel=\\\"next\\\")\", nextLinkRequest.headers['Link'])\n",
    "        if (nextLinkUrl == None):\n",
    "            break\n",
    "            \n",
    "        if (len(nextLinkUrl)):\n",
    "            nextLinkUrl = nextLinkUrl[0]\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        nextLinkRequest = requests.get(nextLinkUrl)\n",
    "        if (nextLinkRequest.ok == True):\n",
    "            jsonData = json.loads(nextLinkRequest.text)\n",
    "            if ('items' in jsonData):\n",
    "                items = jsonData['items']\n",
    "            else:\n",
    "                items = []\n",
    "\n",
    "            for item in items:\n",
    "                repositories.append(item)\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "        \n",
    "        \n",
    "    return repositories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e6f450a5a2bafe89fda0fa4edc6d3b8",
     "grade": true,
     "grade_id": "cell-f9dd722733b1a0b8",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2017-01-15'\n",
    "keyword = 'data science'\n",
    "per_page = 100\n",
    "repositories = collect_repositories(start_date, end_date, keyword, per_page)\n",
    "print(len(repositories))\n",
    "\n",
    "url = f'https://api.github.com/search/repositories?q={keyword} created:{start_date}..{end_date}'\n",
    "finish = False\n",
    "while finish == False:\n",
    "    r = requests.get(url)\n",
    "    if r.ok == True:\n",
    "        json_pydata = r.json()\n",
    "        if json_pydata['incomplete_results'] == False: \n",
    "            total_count = json_pydata['total_count']\n",
    "            finish = True\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "assert len(repositories) == total_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2.2. Thu thập nhiều hơn 1000 thùng chứa (2.5đ)\n",
    "Để thu thập hết 1615 thùng chứa trong ví dụ ở trên, ta sẽ: \n",
    "- Chia khoảng thời gian các thùng chứa được tạo mà ta cần tìm kiếm ra các khoảng thời gian nhỏ hơn. Ví dụ chia 2017-01-01..2017-01-15 thành 2017-01-01..2017-01-07, 2017-01-08..2017-01-14, 2017-01-15..2017-01-15; ở đây, mỗi khoảng thời gian nhỏ gồm 7 ngày (kể cả 2 đầu), khoảng thời gian nhỏ sau cùng có thể sẽ không đủ 7 ngày (do bị lẻ). Trong Python, để làm việc với dữ liệu thời gian, bạn có thể dùng thư viện `datetime`; bạn có thể tự tìm hiểu về thư viện này ở: [document](https://docs.python.org/3.7/library/datetime.html), [Corey's video](https://www.youtube.com/watch?v=eirjjyP2qcQ), Google.\n",
    "- Tìm kiếm với khoảng thời gian nhỏ hơn này bằng hàm `collect_repositories` ở trên (với khoảng thời gian đủ nhỏ sẽ có $\\le1000$ thùng chứa).\n",
    "- Kết hợp kết quả từ các lần tìm kiếm với các khoảng thời gian nhỏ này lại.\n",
    "\n",
    "Bạn sẽ cài đặt các bước này ở hàm `collect_all_repositories` dưới đây. Hàm này có các input:\n",
    "- `start_date`, `end_date`, `keyword`, `per_page`: các input này giống như ở hàm `collect_repositories`.\n",
    "- `step`: là số nguyên, cho biết khoảng thời gian nhỏ gồm bao nhiêu ngày (tính cả 2 đầu). Trong ví dụ ở trên, `step=7`.\n",
    "\n",
    "Output trả về của hàm này:\n",
    "- `all_repositories`: là list tất cả các thùng chứa (\"item\") lấy được."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58dcaf52d67ca4b5a41299b636282ab",
     "grade": false,
     "grade_id": "cell-186ad6493e39e40c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def date_range(start, end, intv):\n",
    "    start = start.replace('-','')\n",
    "    end = end.replace('-','')\n",
    "    start = datetime.strptime(start,\"%Y%m%d\")\n",
    "    end = datetime.strptime(end,\"%Y%m%d\")\n",
    "    diff = (end  - start ) / intv\n",
    "    for i in range(intv):\n",
    "        yield (start + diff * i).strftime(\"%Y%m%d\")\n",
    "    yield end.strftime(\"%Y%m%d\")\n",
    "    \n",
    "def collect_all_repositories(start_date, end_date, step, keyword='data science', per_page=100):\n",
    "    all_repositories = []\n",
    "  \n",
    "    times = list(date_range('2017-01-01', '2017-01-31', step))\n",
    "    \n",
    "    for index in range(len(times)-1):\n",
    "        start = datetime.strptime(times[index], '%Y%m%d').strftime('%Y-%m-%d')\n",
    "        next_range = datetime.strptime(times[index+1], '%Y%m%d')\n",
    "        day = 1\n",
    "        if (index == len(times)-2):\n",
    "            day = 0\n",
    "        end = next_range.date() - timedelta(days=day)\n",
    "   \n",
    "        repositories = collect_repositories(start, end, keyword, per_page)\n",
    "        for item in repositories:\n",
    "            all_repositories.append(item)\n",
    "\n",
    "    \n",
    "    return all_repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e10538857f0a28d84ee3ae2e659cbcc5",
     "grade": true,
     "grade_id": "cell-eb93b1b4dd11e993",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2017-01-31'\n",
    "step = 10\n",
    "keyword = 'data science'\n",
    "per_page = 100\n",
    "all_repositories = collect_all_repositories(start_date, end_date, step, keyword, per_page)\n",
    "print(len(all_repositories))\n",
    "\n",
    "url = f'https://api.github.com/search/repositories?q={keyword} created:{start_date}..{end_date}'\n",
    "finish = False\n",
    "while finish == False:\n",
    "    r = requests.get(url)\n",
    "    if r.ok == True:\n",
    "        json_pydata = r.json()\n",
    "        if json_pydata['incomplete_results'] == False: \n",
    "            total_count = json_pydata['total_count']\n",
    "            finish = True\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "assert len(all_repositories) == total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
